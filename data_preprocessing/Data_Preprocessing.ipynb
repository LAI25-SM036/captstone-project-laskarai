import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
import warnings
warnings.filterwarnings('ignore')
# Load dataset
print("Loading dataset...")
df = pd.read_csv('nilai_urut.csv')
print(f"Dataset shape: {df.shape}")
# Define columns explicitly based on your attributes
all_columns = ['gender', 'absences', 'n_agama', 'n_pkn', 'n_bindo', 'n_mat', 'n_bing',
               'n_por', 'n_bjawa', 'n_kejuruan', 'n_sikap', 'organization', 'extracurricular',
               'origin_jhs', 'address', 'parent_status', 'father_edu', 'father_work',
               'father_salary', 'mother_edu', 'mother_work', 'mother_salary', 'scholarship',
               'family_size', 'house_status', 'final_score']
# Check which columns exist
existing_columns = [col for col in all_columns if col in df.columns]
print(f"\nColumns found: {len(existing_columns)}/{len(all_columns)}")
print(f"Missing columns: {set(all_columns) - set(existing_columns)}")
# Define column types
nilai_columns = ['n_agama', 'n_pkn', 'n_bindo', 'n_mat', 'n_bing', 'n_por', 'n_bjawa', 'n_kejuruan', 'n_sikap']
numeric_columns = ['absences', 'family_size'] + nilai_columns
categorical_columns = ['gender', 'organization', 'extracurricular', 'origin_jhs', 'address',
                      'parent_status', 'father_edu', 'father_work', 'father_salary',
                      'mother_edu', 'mother_work', 'mother_salary', 'scholarship', 'house_status']
target_column = 'final_score'

print("\nData types:")
print(df.dtypes)
# DATA CLEANING
print("\n" + "="*50)
print("DATA CLEANING PROCESS")
print("="*50)

# 1. Convert nilai columns to numeric
print("\nConverting nilai columns to numeric...")
for col in nilai_columns:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
        non_numeric = df[col].isna().sum()
        if non_numeric > 0:
            print(f"{col}: {non_numeric} non-numeric values converted to NaN")

# Also convert other numeric columns
for col in ['absences', 'family_size']:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# 2. Handle missing values
print("\nHandling missing values...")

# For numeric columns - use median
for col in numeric_columns:
    if col in df.columns:
        missing_before = df[col].isna().sum()
        if missing_before > 0:
            median_value = df[col].median()
            df[col].fillna(median_value, inplace=True)
            print(f"{col}: filled {missing_before} missing values with median {median_value:.2f}")

# For categorical columns - use mode
for col in categorical_columns:
    if col in df.columns:
        missing_before = df[col].isna().sum()
        if missing_before > 0:
            mode_value = df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown'
            df[col].fillna(mode_value, inplace=True)
            print(f"{col}: filled {missing_before} missing values with mode '{mode_value}'")

# 3. Remove duplicates
duplicates = df.duplicated().sum()
if duplicates > 0:
    df = df.drop_duplicates()
    print(f"\nRemoved {duplicates} duplicate rows")

# 4. Create rata_rata_nilai (average score)
nilai_cols_present = [col for col in nilai_columns if col in df.columns]
if nilai_cols_present:
    df['rata_rata_nilai'] = df[nilai_cols_present].mean(axis=1)
    print(f"\nCreated rata_rata_nilai from {len(nilai_cols_present)} nilai columns")
    print(f"Rata-rata nilai stats: min={df['rata_rata_nilai'].min():.2f}, max={df['rata_rata_nilai'].max():.2f}, mean={df['rata_rata_nilai'].mean():.2f}")

# 5. Handle outliers (cap method)
print("\nHandling outliers...")
for col in nilai_cols_present + ['absences']:
    if col in df.columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR

        outliers_low = (df[col] < lower).sum()
        outliers_high = (df[col] > upper).sum()

        df[col] = df[col].clip(lower=lower, upper=upper)

        if outliers_low + outliers_high > 0:
            print(f"{col}: capped {outliers_low} low and {outliers_high} high outliers")
# EXPLORATORY DATA ANALYSIS
print("\n" + "="*50)
print("EXPLORATORY DATA ANALYSIS")
print("="*50)

# Statistical summary
numeric_cols_for_summary = [col for col in numeric_columns + ['rata_rata_nilai'] if col in df.columns]
print("\nStatistical Summary:")
print(df[numeric_cols_for_summary].describe())

# Correlation matrix
plt.figure(figsize=(12, 10))
# Calculate correlation matrix
corr_matrix = df[numeric_cols_for_summary].corr()

# Create heatmap without mask to show all correlations
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
            fmt='.2f', square=True, linewidths=0.5,
            cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix of Numeric Features', fontsize=14, pad=20)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
plt.close()
print("\nCorrelation matrix saved")

# Alternative: Show only lower triangle if you prefer
plt.figure(figsize=(12, 10))
# Create mask for upper triangle
mask = np.zeros_like(corr_matrix, dtype=bool)
mask[np.triu_indices_from(mask, k=1)] = True

# Create heatmap with lower triangle only
sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
            fmt='.2f', square=True, linewidths=0.5,
            cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix of Numeric Features (Lower Triangle)', fontsize=14, pad=20)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('correlation_matrix_triangle.png', dpi=300, bbox_inches='tight')
plt.close()
print("Correlation matrix (triangle) saved")

# Distribution plots for nilai
n_nilai = len(nilai_cols_present)
if n_nilai > 0:
    cols_per_row = 3
    rows = (n_nilai + cols_per_row - 1) // cols_per_row

    fig, axes = plt.subplots(rows, cols_per_row, figsize=(15, 4*rows))
    # Handle single row case
    if rows == 1:
        axes = [axes] if cols_per_row == 1 else axes
    else:
        axes = axes.flatten()

    for idx, col in enumerate(nilai_cols_present):
        ax = axes[idx]
        # Remove NaN values before plotting
        data = df[col].dropna()
        if len(data) > 0:
            ax.hist(data, bins=20, edgecolor='black', alpha=0.7, color='skyblue')
            ax.axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data.mean():.2f}')
            ax.set_title(f'Distribution of {col}')
            ax.set_xlabel('Score')
            ax.set_ylabel('Frequency')
            ax.legend()
        else:
            ax.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'Distribution of {col}')

    # Hide unused subplots
    for idx in range(n_nilai, len(axes)):
        axes[idx].set_visible(False)

    plt.tight_layout()
    plt.savefig('nilai_distributions.png', dpi=300)
    plt.close()
    print("Distribution plots saved")

# Print correlation insights
print("\nCorrelation Insights:")
if 'rata_rata_nilai' in corr_matrix.columns:
    # Get correlations with rata_rata_nilai
    corr_with_target = corr_matrix['rata_rata_nilai'].sort_values(ascending=False)
    print("\nTop correlations with rata_rata_nilai:")
    # Exclude self-correlation
    for feature, corr_value in corr_with_target[1:6].items():
        print(f"  {feature}: {corr_value:.3f}")

    print("\nLowest correlations with rata_rata_nilai:")
    for feature, corr_value in corr_with_target[-5:].items():
        print(f"  {feature}: {corr_value:.3f}")
# FEATURE ENCODING
print("\n" + "="*50)
print("FEATURE ENCODING")
print("="*50)

# Create encoded dataframe
df_encoded = df.copy()

# 1. Binary encoding
binary_mappings = {
    'gender': {'L': 0, 'P': 1, 'Laki-laki': 0, 'Perempuan': 1, 'Male': 0, 'Female': 1},
    'organization': {'Ya': 1, 'Tidak': 0, 'Y': 1, 'T': 0, 'yes': 1, 'no': 0},
    'extracurricular': {'Ya': 1, 'Tidak': 0, 'Y': 1, 'T': 0, 'yes': 1, 'no': 0},
    'scholarship': {'Ya': 1, 'Tidak': 0, 'Y': 1, 'T': 0, 'yes': 1, 'no': 0}
}

for col, mapping in binary_mappings.items():
    if col in df_encoded.columns:
        # Try mapping first
        df_encoded[col] = df_encoded[col].map(mapping)
        # If still has non-numeric, use label encoder
        if df_encoded[col].dtype == 'object':
            le = LabelEncoder()
            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))
        print(f"{col}: binary encoded")

# 2. Ordinal encoding for education
edu_mapping = {
    'SD': 1, 'SMP': 2, 'SMA': 3, 'SMK': 3,
    'D1': 4, 'D2': 4, 'D3': 4,
    'S1': 5, 'S2': 6, 'S3': 7
}

for col in ['father_edu', 'mother_edu']:
    if col in df_encoded.columns:
        df_encoded[col] = df_encoded[col].map(edu_mapping)
        df_encoded[col].fillna(3, inplace=True)  # Default to SMA level
        df_encoded[col] = df_encoded[col].astype(int)
        print(f"{col}: ordinal encoded (education)")

# 3. Ordinal encoding for salary
salary_mapping = {
    '<1jt': 1, '< 1 juta': 1, '<1 juta': 1,
    '1-3jt': 2, '1-3 juta': 2, '1 - 3 juta': 2,
    '3-5jt': 3, '3-5 juta': 3, '3 - 5 juta': 3,
    '5-10jt': 4, '5-10 juta': 4, '5 - 10 juta': 4,
    '>10jt': 5, '> 10 juta': 5, '>10 juta': 5
}

for col in ['father_salary', 'mother_salary']:
    if col in df_encoded.columns:
        df_encoded[col] = df_encoded[col].map(salary_mapping)
        df_encoded[col].fillna(2, inplace=True)  # Default to 1-3jt
        df_encoded[col] = df_encoded[col].astype(int)
        print(f"{col}: ordinal encoded (salary)")

# 4. Label encoding for remaining categorical
remaining_categorical = ['origin_jhs', 'address', 'parent_status', 'father_work',
                        'mother_work', 'house_status']

for col in remaining_categorical:
    if col in df_encoded.columns and df_encoded[col].dtype == 'object':
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))
        print(f"{col}: label encoded")

# 5. Handle final_score if it's categorical
if 'final_score' in df_encoded.columns and df_encoded['final_score'].dtype == 'object':
    le = LabelEncoder()
    df_encoded['final_score'] = le.fit_transform(df_encoded['final_score'].astype(str))
    print("final_score: label encoded")

# Verify all columns are numeric
print("\nVerifying encoding...")
for col in df_encoded.columns:
    if df_encoded[col].dtype == 'object':
        print(f"Warning: {col} is still non-numeric, applying label encoding...")
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))
# FEATURE SCALING
print("\n" + "="*50)
print("FEATURE SCALING")
print("="*50)

df_scaled = df_encoded.copy()
scaler = StandardScaler()

# Scale nilai columns and rata_rata_nilai
cols_to_scale = nilai_cols_present + ['rata_rata_nilai']
if cols_to_scale:
    df_scaled[cols_to_scale] = scaler.fit_transform(df_scaled[cols_to_scale])
    print(f"Scaled {len(cols_to_scale)} nilai-related columns")

# SAVE RESULTS
print("\n" + "="*50)
print("SAVING RESULTS")
print("="*50)

# Save processed data
df_encoded.to_csv('datasmk2_processed_encoded.csv', index=False)
print("✓ Saved: datasmk2_processed_encoded.csv")

df_scaled.to_csv('datasmk2_processed_scaled.csv', index=False)
print("✓ Saved: datasmk2_processed_scaled.csv")

# Create report
with open('data_processing_report.txt', 'w') as f:
    f.write("DATA PROCESSING REPORT\n")
    f.write("="*60 + "\n\n")
    f.write(f"Processing Date: {pd.Timestamp.now()}\n\n")
    f.write(f"Original shape: {df.shape}\n")
    f.write(f"Final shape: {df_encoded.shape}\n\n")
    f.write("FEATURES:\n")
    f.write(f"- Nilai columns: {nilai_cols_present}\n")
    f.write(f"- Target for regression: rata_rata_nilai\n")
    f.write(f"- Target for classification: final_score\n\n")
    f.write("COLUMNS AFTER PROCESSING:\n")
    for col in sorted(df_encoded.columns):
        f.write(f"- {col}: {df_encoded[col].dtype}\n")

print("✓ Saved: data_processing_report.txt")

# Final summary
print("\n" + "="*50)
print("PROCESSING COMPLETE!")
print("="*50)
print(f"\nDataset shape: {df_encoded.shape}")
print(f"All columns numeric: {all(df_encoded[col].dtype in ['int64', 'float64'] for col in df_encoded.columns)}")

if 'rata_rata_nilai' in df_encoded.columns:
    print(f"\nRata-rata nilai range: {df_encoded['rata_rata_nilai'].min():.2f} - {df_encoded['rata_rata_nilai'].max():.2f}")

if 'final_score' in df_encoded.columns:
    print(f"\nFinal score unique values: {df_encoded['final_score'].nunique()}")

print("\nFiles created:")
print("1. datasmk2_processed_encoded.csv - For ML modeling")
print("2. datasmk2_processed_scaled.csv - With scaled features")
print("3. correlation_matrix.png")
print("4. nilai_distributions.png")
print("5. data_processing_report.txt")
# Additional analysis and validation
print("\n" + "="*50)
print("DATA VALIDATION & ADDITIONAL ANALYSIS")
print("="*50)

# Check for any remaining issues
print("\nData Quality Check:")
print(f"- Missing values: {df_encoded.isnull().sum().sum()}")
print(f"- Duplicate rows: {df_encoded.duplicated().sum()}")
print(f"- Non-numeric columns: {len(df_encoded.select_dtypes(exclude=[np.number]).columns)}")

# Feature correlation with targets
if 'rata_rata_nilai' in df_encoded.columns:
    print("\nTop 10 features correlated with rata_rata_nilai:")
    correlations = df_encoded.corr()['rata_rata_nilai'].sort_values(ascending=False)
    print(correlations[1:11])  # Exclude self-correlation

if 'final_score' in df_encoded.columns:
    print("\nFinal score distribution:")
    print(df_encoded['final_score'].value_counts().sort_index())

# Create feature importance visualization
if 'rata_rata_nilai' in df_encoded.columns:
    plt.figure(figsize=(10, 8))

    # Get correlations excluding rata_rata_nilai itself
    feature_cols = [col for col in df_encoded.columns if col != 'rata_rata_nilai']
    correlations = df_encoded[feature_cols].corrwith(df_encoded['rata_rata_nilai']).sort_values(ascending=True)

    # Plot horizontal bar chart
    colors = ['red' if x < 0 else 'green' for x in correlations]
    correlations.plot(kind='barh', color=colors, figsize=(10, 8))
    plt.title('Feature Correlations with Rata-rata Nilai')
    plt.xlabel('Correlation Coefficient')
    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300)
    plt.close()
    print("\n✓ Saved: feature_importance.png")

# Create summary statistics for each feature group
print("\n" + "="*50)
print("FEATURE GROUP STATISTICS")
print("="*50)

# Academic performance
if nilai_cols_present:
    print("\nAcademic Performance (Nilai):")
    academic_stats = df_encoded[nilai_cols_present].describe().T[['mean', 'std', 'min', 'max']]
    print(academic_stats.round(2))

# Family background
family_features = ['father_edu', 'mother_edu', 'father_salary', 'mother_salary', 'family_size']
family_features_present = [col for col in family_features if col in df_encoded.columns]
if family_features_present:
    print("\nFamily Background:")
    family_stats = df_encoded[family_features_present].describe().T[['mean', 'std', 'min', 'max']]
    print(family_stats.round(2))

# Student activities
activity_features = ['absences', 'organization', 'extracurricular']
activity_features_present = [col for col in activity_features if col in df_encoded.columns]
if activity_features_present:
    print("\nStudent Activities:")
    for col in activity_features_present:
        if col == 'absences':
            print(f"  {col}: mean={df_encoded[col].mean():.2f}, std={df_encoded[col].std():.2f}")
        else:
            print(f"  {col}: {df_encoded[col].value_counts().to_dict()}")

# Save sample data for verification
print("\n" + "="*50)
print("SAVING SAMPLE DATA")
print("="*50)
# Save first 10 rows as sample
df_encoded.head(10).to_csv('datasmk2_sample_encoded.csv', index=False)
print("✓ Saved: datasmk2_sample_encoded.csv (first 10 rows for verification)")

# Create a data dictionary
data_dict = pd.DataFrame({
    'Column': df_encoded.columns,
    'Type': df_encoded.dtypes,
    'Non_Null_Count': df_encoded.count(),
    'Null_Count': df_encoded.isnull().sum(),
    'Unique_Values': df_encoded.nunique(),
    'Min': df_encoded.min(),
    'Max': df_encoded.max(),
    'Mean': df_encoded.mean().round(2)
})

data_dict.to_csv('data_dictionary.csv', index=False)
print("✓ Saved: data_dictionary.csv")

# Final checks
print("\n" + "="*50)
print("FINAL DATA QUALITY REPORT")
print("="*50)

print(f"\nProcessed dataset summary:")
print(f"- Total records: {len(df_encoded)}")
print(f"- Total features: {len(df_encoded.columns)}")
print(f"- Numeric features: {len(df_encoded.select_dtypes(include=[np.number]).columns)}")
print(f"- Memory usage: {df_encoded.memory_usage().sum() / 1024**2:.2f} MB")

# Check data balance for classification
if 'final_score' in df_encoded.columns:
    print("\nClass balance for final_score:")
    class_counts = df_encoded['final_score'].value_counts()
    for class_val, count in class_counts.items():
        percentage = (count / len(df_encoded)) * 100
        print(f"  Class {class_val}: {count} samples ({percentage:.1f}%)")
# Recommendations
print("\n" + "="*50)
print("RECOMMENDATIONS FOR MODELING")
print("="*50)

print("\nFor Regression (predicting rata_rata_nilai):")
print("- Use datasmk2_processed_scaled.csv for algorithms sensitive to scale (SVM, Neural Networks)")
print("- Use datasmk2_processed_encoded.csv for tree-based algorithms (Random Forest, XGBoost)")
print("- Consider feature selection based on correlation analysis")

print("\nFor Classification (predicting final_score):")
print("- Check class balance and consider stratified sampling")
print("- Use appropriate metrics (accuracy, precision, recall, F1)")
print("- Consider SMOTE or class weights for imbalanced data")

print("\nFeature Engineering Suggestions:")
print("- Create interaction features between parent education and student performance")
print("- Bin continuous variables like absences into categories")
print("- Create aggregate features like total_parent_education")
